{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On many occasions, while working with the scikit-learn library, you'll need to save your prediction models to file, and then restore them in order to reuse your previous work to: test your model on new data, compare multiple models, or anything else. \n",
    "\n",
    "### This saving procedure is also known as object serialization - representing an object with a stream of bytes, in order to store it on disk, send it over a network or save to a database, while the restoring procedure is known as deserialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 3 methods to do so - Pickle, Joblib and Manual JSON dump. None of these approaches represents an optimal solution, but the right fit should be chosen according to the needs of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, let's create one scikit-learn model. \n",
    "# We'll use a Logistic Regression model and the Iris dataset. \n",
    "# Let's import the needed libraries, load the data, and split it in training and test sets.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "data = load_iris()\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data.data, data.target, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create the model with some non-default parameters and fit it to the training data. We assume that you have previously found the optimal parameters of the model, i.e. the ones which produce highest estimated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew Wong\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=20, n_jobs=3, solver='liblinear')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model\n",
    "model = LogisticRegression(C=0.1, \n",
    "                           max_iter=20, \n",
    "                           fit_intercept=True, \n",
    "                           n_jobs=3, \n",
    "                           solver='liblinear')\n",
    "model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=20, multi_class='ovr', n_jobs=3,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And our resulting model\n",
    "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "    intercept_scaling=1, max_iter=20, multi_class='ovr', n_jobs=3,\n",
    "    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "    verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fit method, the model has learned its coefficients which are stored in model.coef_. The goal is to save the model's parameters and coefficients to file, so you don't need to repeat the model training and parameter optimization steps again on new data.\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following few lines of code, the model which we created in the previous step is saved to file, and then loaded as a new object called pickled_model. The loaded model is then used to calculate the accuracy score and predict outcomes on new unseen (test) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 91.11 %\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "#\n",
    "# Create your model here (same as above)\n",
    "#\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pickle_model_logistic_regression.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(Xtest, Ytest)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "Ypredict = pickle_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Joblib\n",
    "\n",
    "## ----------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 91.11 %\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"joblib_model.pkl\"\n",
    "joblib.dump(model, joblib_file)\n",
    "\n",
    "# Load from file\n",
    "joblib_model = joblib.load(joblib_file)\n",
    "\n",
    "# Calculate the accuracy and predictions\n",
    "score = joblib_model.score(Xtest, Ytest)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "Ypredict = pickle_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As seen from the example, the Joblib library offers a bit simpler workflow compared to Pickle. While Pickle requires a file object to be passed as an argument, Joblib works with both file objects and string filenames. In case your model contains large arrays of data, each array will be stored in a separate file, but the save and restore procedure will remain the same. Joblib also allows different compression methods, such as 'zlib', 'gzip', 'bz2', and different levels of compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Manual Save & Restore to JSON\n",
    "\n",
    "## ----------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This approach allows us to select the data which needs to be saved, such as the model parameters, coefficients, training data, and anything else we need.\n",
    "\n",
    "### Since we want to save all of this data in a single object, one possible way to do it is to create a new class which inherits from the model class, which in our example is LogisticRegression. The new class, called MyLogReg, then implements the methods save_json and load_json for saving and restoring to/from a JSON file, respectively.\n",
    "\n",
    "### For simplicity, we'll save only three model parameters and the training data. Some additional data we could store with this approach is, for example, a cross-validation score on the training set, test data, accuracy score on the test data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class MyLogReg(LogisticRegression):\n",
    "    \n",
    "    # Override the class constructor\n",
    "    def __init__(self, C=1.0, solver='liblinear', max_iter=100, X_train=None, Y_train=None):\n",
    "        LogisticRegression.__init__(self, C=C, solver=solver, max_iter=max_iter)\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "    # A method for saving object data to JSON file\n",
    "    def save_json(self, filepath):\n",
    "        dict_ = {}\n",
    "        dict_['C'] = self.C\n",
    "        dict_['max_iter'] = self.max_iter\n",
    "        dict_['solver'] = self.solver\n",
    "        dict_['X_train'] = self.X_train.tolist() if self.X_train is not None else 'None'\n",
    "        dict_['Y_train'] = self.Y_train.tolist() if self.Y_train is not None else 'None'\n",
    "        \n",
    "        # Creat json and save to file\n",
    "        json_txt = json.dumps(dict_, indent=4)\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(json_txt)\n",
    "    \n",
    "    # A method for loading data from JSON file\n",
    "    def load_json(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            dict_ = json.load(file)\n",
    "            \n",
    "        self.C = dict_['C']\n",
    "        self.max_iter = dict_['max_iter']\n",
    "        self.solver = dict_['solver']\n",
    "        self.X_train = np.asarray(dict_['X_train']) if dict_['X_train'] != 'None' else None\n",
    "        self.Y_train = np.asarray(dict_['Y_train']) if dict_['Y_train'] != 'None' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try the MyLogReg class. First we create an object mylogreg, pass the training data to it, and save it to file. Then we create a new object json_mylogreg and call the load_json method to load the data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLogReg(X_train=array([[4.3, 3. , 1.1, 0.1],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6.4, 2.9, 4...\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.7, 2.8, 6.7, 2. ]]),\n",
       "         Y_train=array([0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2,\n",
       "       1, 0, 2, 2, 0, 1, 2, 0, 2, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 1, 2,\n",
       "       1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1,\n",
       "       1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 0, 0, 2, 1, 0, 0, 2,\n",
       "       1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"mylogreg.json\"\n",
    "\n",
    "# Create a model and train it\n",
    "mylogreg = MyLogReg(X_train=Xtrain, Y_train=Ytrain)\n",
    "mylogreg.save_json(filepath)\n",
    "\n",
    "# Create a new object and load its data from JSON file\n",
    "json_mylogreg = MyLogReg()\n",
    "json_mylogreg.load_json(filepath)\n",
    "json_mylogreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data serialization using JSON actually saves the object into a string format, rather than byte stream, the 'mylogreg.json' file could be opened and modified with a text editor. Although this approach would be convenient for the developer, it is less secure since an intruder can view and amend the content of the JSON file. Moreover, this approach is more suitable for objects with small number of instance variables, such as the scikit-learn models, because any addition of new variables requires changes in the save and restore methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
